{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b554e55-6654-4815-8a68-66511847deaf",
   "metadata": {},
   "source": [
    "Using reinforcement learning to optimize decision-making strategies for quantum circuit design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31d9459a-5f98-42a6-b497-a81a38f07896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from qiskit import QuantumCircuit, transpile\n",
    "from qiskit_aer import Aer\n",
    "from qiskit.circuit.library import HGate, CXGate, SGate, TGate, XGate, YGate, ZGate, CRZGate, TdgGate, UnitaryGate\n",
    "from qiskit.quantum_info import Operator\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041e528a",
   "metadata": {},
   "source": [
    "# **Define Matrices and Operators**\n",
    "\n",
    "This section defines key quantum operators, unitary transformations, and quantum circuits for Bell states, GHZ states, and textbook examples.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51c38e94-fa02-4dc2-95f7-d70df2775aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define basic Quantum gates\n",
    "\n",
    "H = np.array([[1, 1], [1, -1]]) / np.sqrt(2)\n",
    "X = np.array([[0, 1], [1, 0]])\n",
    "Z = np.array([[1, 0], [0, -1]])\n",
    "\n",
    "# Define matrices and operators\n",
    "swap_matrix = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "CNOT = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 0, 1],\n",
    "    [0, 0, 1, 0]\n",
    "])\n",
    "\n",
    "# Bell state unitary\n",
    "bell_state_unitary = Operator(CNOT) @ Operator(np.kron(H, np.eye(2)))\n",
    "phi_minus = Operator(np.kron(np.eye(2), Z)) @ Operator(CNOT) @ Operator(np.kron(H, np.eye(2)))\n",
    "psi_plus = Operator(CNOT) @ Operator(np.kron(X, np.eye(2))) @ Operator(np.kron(H, np.eye(2)))\n",
    "psi_minus = Operator(np.kron(np.eye(2), Z)) @ Operator(CNOT) @ Operator(np.kron(X, np.eye(2))) @ Operator(np.kron(H, np.eye(2)))\n",
    "\n",
    "# CZ matrix\n",
    "cz_matrix = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 0, 0, -1]\n",
    "])\n",
    "\n",
    "# GHZ Circuit (3 qubits)\n",
    "ghz_circuit = QuantumCircuit(3)\n",
    "ghz_circuit.h(0)\n",
    "ghz_circuit.cx(0, 1)\n",
    "ghz_circuit.cx(1, 2)\n",
    "ghz_circuit = Operator(ghz_circuit)\n",
    "\n",
    "# Textbook circuits\n",
    "# page 200\n",
    "text_circuit1 = QuantumCircuit(3)\n",
    "text_circuit1.cx(0,1)\n",
    "text_circuit1.cx(1,2)\n",
    "text_circuit1.h(0)\n",
    "text_circuit1.h(1)\n",
    "text_circuit1.h(2)\n",
    "text_circuit1 = Operator(text_circuit1)\n",
    "\n",
    "iswap_matrix = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 0, 1j, 0],\n",
    "    [0, 1j, 0, 0],\n",
    "    [0, 0, 0, 1]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82345c3-9bea-4c05-a858-e139d1584dc9",
   "metadata": {},
   "source": [
    "You can make your own circuit here and modify and test the effect of Q learning reinforcement learning algorithm in designing the circuit in the subsequent code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623cd96d-6c9f-46d6-b7cd-9dc9da921114",
   "metadata": {},
   "source": [
    "# **How to Create Your Own Quantum Circuit**\n",
    "\n",
    "Quantum computing circuits are designed using `Qiskit`, a quantum computing framework in Python. Below is a step-by-step guide on how to create your own quantum circuit.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Import Necessary Libraries**\n",
    "Before creating a quantum circuit, you need to import the required libraries.\n",
    "\n",
    "```python\n",
    "from qiskit import QuantumCircuit, Aer, transpile, assemble, execute\n",
    "from qiskit.quantum_info import Operator\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Create a Quantum Circuit**\n",
    "You can create a quantum circuit using `QuantumCircuit`. The number of qubits is specified as an argument.\n",
    "\n",
    "```python\n",
    "qc = QuantumCircuit(2)  # Create a quantum circuit with 2 qubits\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Apply Quantum Gates**\n",
    "Quantum gates manipulate qubits in different ways. Some commonly used quantum gates include:\n",
    "\n",
    "- **Hadamard Gate (H)**: Creates a superposition state.\n",
    "- **CNOT Gate (CX)**: Entangles two qubits.\n",
    "- **Pauli Gates (X, Y, Z)**: Represent basic quantum operations.\n",
    "\n",
    "Example of applying gates:\n",
    "\n",
    "```python\n",
    "qc.h(0)        # Apply Hadamard gate to qubit 0\n",
    "qc.cx(0, 1)    # Apply CNOT gate with qubit 0 as control and qubit 1 as target\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Convert the Circuit to a Matrix**\n",
    "To obtain the matrix representation of a circuit, use the `Operator` class:\n",
    "\n",
    "```python\n",
    "unitary_operator = Operator(qc)\n",
    "print(unitary_operator.data)  # Print the corresponding unitary matrix\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Check If a Matrix is Unitary**\n",
    "You make create a np.array to have your own Quantum Circuit.\n",
    "But have to make sure that it is a Unitary.\n",
    "\n",
    "\n",
    "### **Function to Check If a Matrix is Unitary**\n",
    "```python\n",
    "def is_unitary(matrix):\n",
    "    \"\"\"Check if a given np.array is a unitary matrix.\"\"\"\n",
    "    identity = np.eye(matrix.shape[0])  # Create an identity matrix of the same size\n",
    "    conjugate_transpose = np.conjugate(matrix).T  # Compute conjugate transpose\n",
    "    return np.allclose(identity, conjugate_transpose @ matrix) and np.allclose(identity, matrix @ conjugate_transpose)\n",
    "\n",
    "# Example matrices\n",
    "matrix1 = np.array([\n",
    "    [1, 0],\n",
    "    [0, -1]\n",
    "])  # Unitary matrix (Z gate)\n",
    "\n",
    "matrix2 = np.array([\n",
    "    [1, 1],\n",
    "    [1, 1]\n",
    "])  # Not a unitary matrix\n",
    "\n",
    "print(is_unitary(matrix1))  # True\n",
    "print(is_unitary(matrix2))  # False\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Test If Quantum Circuit Matrices Are Unitary**\n",
    "You can test whether predefined matrices (e.g., `swap_matrix`, `CNOT`, `cz_matrix`) are unitary:\n",
    "\n",
    "```python\n",
    "print(is_unitary(swap_matrix))  # Should return True\n",
    "print(is_unitary(CNOT))  # Should return True\n",
    "print(is_unitary(cz_matrix))  # Should return True\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec0e69f-a1f3-4dd4-8a51-9dcfc3692eb0",
   "metadata": {},
   "source": [
    "Hash function for Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f4b55a-da5e-40a9-82c1-de906bf5c614",
   "metadata": {},
   "source": [
    "# **Matrix Hashing and Unique ID Assignment**\n",
    "\n",
    "This script provides a mechanism to assign unique IDs to matrices by hashing them into a dictionary. The purpose of this approach is to efficiently track and identify matrices without redundant storage.\n",
    "\n",
    "## **How It Works**\n",
    "1. **Matrix Hashing:**  \n",
    "   - The function `matrix_to_hash(matrix)` converts a given matrix into a hashable tuple format.  \n",
    "   - This ensures that matrices can be used as dictionary keys.\n",
    "\n",
    "2. **Unique ID Assignment:**  \n",
    "   - The function `get_matrix_id(matrix)` checks if a given matrix has been previously encountered.  \n",
    "   - If the matrix is new, it is assigned a unique ID and stored in `matrix_dict`.  \n",
    "   - If the matrix already exists in the dictionary, its previously assigned ID is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ebaf3c1-400f-4000-b6b3-3c1560588c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store unique matrix hashes and their corresponding IDs\n",
    "matrix_dict = {}\n",
    "counter = 0 \n",
    "\n",
    "def matrix_to_hash(matrix):\n",
    "    \"\"\"\n",
    "    Convert a matrix to a hashable tuple format.\n",
    "    \"\"\"\n",
    "    matrix_array = np.asarray(matrix) \n",
    "    return tuple(tuple(row) for row in matrix_array)\n",
    "\n",
    "def get_matrix_id(matrix):\n",
    "    \"\"\"\n",
    "    Assign a unique ID to a matrix if it has not been encountered before.\n",
    "    \"\"\"\n",
    "    global counter\n",
    "    matrix_hash = matrix_to_hash(matrix)\n",
    "    \n",
    "    if matrix_hash not in matrix_dict:\n",
    "        matrix_dict[matrix_hash] = counter\n",
    "        counter += 1  \n",
    "    \n",
    "    return matrix_dict[matrix_hash]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc79e19-67ca-411e-895d-49eaadef258e",
   "metadata": {},
   "source": [
    "# **1:Quantum Environment for Reinforcement Learning**\n",
    "\n",
    "This class is an implementation of a quantum environment using `OpenAI Gym` and `Qiskit`. The environment is designed for reinforcement learning (RL) tasks, where the goal is to apply quantum operations to match a target unitary transformation.\n",
    "\n",
    "---\n",
    "\n",
    "## **Overview**\n",
    "The environment simulates a **two-qubit quantum circuit**, where an agent applies quantum gates to reach a target unitary matrix. The circuit's state evolves as actions (quantum gates) are applied, and a reward function evaluates how close the resulting unitary matrix is to the target.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4012e89e-ce2c-4990-b0dd-3e6c293573cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(QuantumEnv, self).__init__()\n",
    "        \n",
    "        # Set the number of qubits\n",
    "        self.num_qubits = 2\n",
    "        # Initialize the quantum circuit\n",
    "        self.circuit = QuantumCircuit(self.num_qubits)\n",
    "        # Set the target unitary matrix (can be changed to bell_state, cz, swap, iswap)\n",
    "        self.target_unitary = bell_state_unitary  \n",
    "        \n",
    "        # Define the action space (6 possible actions)\n",
    "        self.action_space = spaces.Discrete(6)\n",
    "        # Define the observation space (100 possible state hashes)\n",
    "        self.observation_space = spaces.Discrete(100)\n",
    "        \n",
    "        # Mapping of state indices\n",
    "        self.state_to_index = {}\n",
    "        self.index_to_state = []\n",
    "\n",
    "    def _hash_circuit(self, circuit: QuantumCircuit) -> int:\n",
    "        \"\"\"\n",
    "        Compute a hash value for the given quantum circuit.\n",
    "        \"\"\"\n",
    "        matrix = Operator(circuit)  # Get the unitary matrix of the circuit\n",
    "        return get_matrix_id(matrix) % 100  # Compute hash value within 100\n",
    "\n",
    "    def get_state_index(self, state: QuantumCircuit) -> int:\n",
    "        \"\"\"\n",
    "        Get the index of a state; if it is a new state, add it to the index mapping.\n",
    "        \"\"\"\n",
    "        state_hash = self._hash_circuit(state)\n",
    "        if state_hash not in self.state_to_index:\n",
    "            index = len(self.state_to_index)\n",
    "            self.state_to_index[state_hash] = index\n",
    "            self.index_to_state.append(state)\n",
    "        return self.state_to_index[state_hash]\n",
    "\n",
    "    def get_state_from_index(self, index: int) -> QuantumCircuit:\n",
    "        \"\"\"\n",
    "        Retrieve the quantum circuit state based on the index.\n",
    "        \"\"\"\n",
    "        if 0 <= index < len(self.index_to_state):\n",
    "            return self.index_to_state[index]\n",
    "        return None\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment and return the initial state index.\n",
    "        \"\"\"\n",
    "        self.circuit = QuantumCircuit(self.num_qubits)  # Reinitialize the circuit\n",
    "        return self.get_state_index(self.circuit)\n",
    "\n",
    "    def step(self, action, qubits):\n",
    "        \"\"\"\n",
    "        Execute an action, update the environment state, and compute the reward.\n",
    "        \"\"\"\n",
    "        self.circuit.append(action, qubits)  # Append the action to the circuit\n",
    "        state_index = self.get_state_index(self.circuit)  # Get the new state index\n",
    "        reward, done = self._reward(self.target_unitary)  # Compute the reward\n",
    "        return state_index, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Render the quantum circuit.\n",
    "        \"\"\"\n",
    "        print(self.circuit.draw())\n",
    "\n",
    "    def _reward(self, target_unitary):\n",
    "        \"\"\"\n",
    "        Compute the fidelity between the circuit and the target unitary matrix and return the reward.\n",
    "        \"\"\"\n",
    "        simulator = Aer.get_backend('unitary_simulator')  # Get the unitary simulator\n",
    "        result = simulator.run(transpile(self.circuit, simulator)).result()\n",
    "        unitary = result.get_unitary(self.circuit)  # Get the unitary matrix of the current circuit\n",
    "        \n",
    "        # Compute the fidelity of the quantum state\n",
    "        unitary_array = np.asarray(unitary)\n",
    "        target_unitary_array = np.asarray(target_unitary)\n",
    "        fidelity = np.abs(np.trace(unitary_array.conj().T @ target_unitary_array)) / (2 ** self.num_qubits)\n",
    "        \n",
    "        reward = 0\n",
    "        done = False\n",
    "        if fidelity > 0.99:\n",
    "            done = True  # Task completed\n",
    "            reward += 100  # Assign high reward\n",
    "            self.render()  # Display the final circuit\n",
    "        return reward, done\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Close the environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Display the quantum circuit.\n",
    "        \"\"\"\n",
    "        print(self.circuit.draw())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd00fe4-3584-4269-aaf9-573a1fcf95fd",
   "metadata": {},
   "source": [
    "## **Overview**\n",
    "The advantage of the Reverse construction is that it unifies the training objectives so that different gate sequences all point to the same fixed end point, thereby improving the generalization ability of the model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6fcfe8-98a8-4d3e-bd6b-2ebba8ad69c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(QuantumEnv, self).__init__()\n",
    "\n",
    "        self.num_qubits = 2  # Number of qubits\n",
    "        self.circuit = ResetCircuit()  # Initialize the circuit\n",
    "        self.target_unitary = Operator(QuantumCircuit(self.num_qubits))  # Target gate (can be replaced)\n",
    "\n",
    "        self.action_space = spaces.Discrete(6)  # 6 possible actions\n",
    "        self.observation_space = spaces.Discrete(100)  # 100 hashed circuit states\n",
    "\n",
    "        self.state_to_index = {}  # Maps hashed circuits to index\n",
    "        self.index_to_state = []  # Stores circuits for reverse lookup\n",
    "\n",
    "    def _hash_circuit(self, circuit: QuantumCircuit) -> int:\n",
    "        # Convert circuit to unitary and hash it to get state ID\n",
    "        matrix = Operator(circuit)\n",
    "        return get_matrix_id(matrix) % 100\n",
    "\n",
    "    def get_state_index(self, state: QuantumCircuit) -> int:\n",
    "        # Return index for a circuit; add to list if new\n",
    "        state_hash = self._hash_circuit(state)\n",
    "        if state_hash not in self.state_to_index:\n",
    "            index = len(self.state_to_index)\n",
    "            self.state_to_index[state_hash] = index\n",
    "            self.index_to_state.append(state)\n",
    "        return self.state_to_index[state_hash]\n",
    "\n",
    "    def get_state_from_index(self, index: int) -> QuantumCircuit:\n",
    "        # Retrieve a circuit from its index\n",
    "        if 0 <= index < len(self.index_to_state):\n",
    "            return self.index_to_state[index]\n",
    "        return None\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset circuit and return the initial state index\n",
    "        self.circuit = ResetCircuit()\n",
    "        return self.get_state_index(self.circuit)\n",
    "\n",
    "    def step(self, action, qubits):\n",
    "        # Apply action to the circuit and return new state, reward, and done\n",
    "        self.circuit.append(action, qubits)\n",
    "        state_index = self.get_state_index(self.circuit)\n",
    "        reward, done = self._reward(self.target_unitary)\n",
    "        return state_index, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        # Print the current circuit diagram\n",
    "        print(self.circuit.draw())\n",
    "\n",
    "    def _reward(self, target_unitary):\n",
    "        # Compare circuit with target; reward if fidelity is high\n",
    "        simulator = Aer.get_backend('unitary_simulator')\n",
    "        result = simulator.run(transpile(self.circuit, simulator)).result()\n",
    "        unitary = result.get_unitary(self.circuit)\n",
    "\n",
    "        unitary_array = np.asarray(unitary)\n",
    "        target_unitary_array = np.asarray(target_unitary)\n",
    "\n",
    "        fidelity = np.abs(np.trace(unitary_array.conj().T @ target_unitary_array)) / (2 ** self.num_qubits)\n",
    "\n",
    "        reward = 0\n",
    "        done = False\n",
    "        if fidelity > 0.99:\n",
    "            done = True\n",
    "            reward += 100\n",
    "            self.render()\n",
    "        return reward, done\n",
    "\n",
    "    def close(self):\n",
    "        # Optional cleanup method (not used)\n",
    "        pass\n",
    "\n",
    "    def render(self):\n",
    "        # Repeated render method (prints circuit again)\n",
    "        print(self.circuit.draw())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033b0861-b656-4d25-a50e-5f1a780ecdac",
   "metadata": {},
   "source": [
    "# **2:Q-learning Agent for Quantum Reinforcement Learning**\n",
    "\n",
    "This class is a **Q-learning agent** designed for reinforcement learning in a quantum environment. The agent learns how to construct quantum circuits by selecting quantum gates to maximize a reward function.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c993060c-f3a6-4fcf-a3f1-f4ef1f0207ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-learning agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, state_size, action_size, alpha, gamma, epsilon, decay_rate, epsilon_min):\n",
    "        \"\"\"\n",
    "        Initialize the Q-learning agent with given parameters.\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.decay_rate = decay_rate  # Decay rate for epsilon\n",
    "        self.epsilon_min = epsilon_min  # Minimum value of epsilon\n",
    "        self.q_table = np.zeros((state_size, action_size))  # Initialize Q-table with zeros\n",
    "    \n",
    "    def choose_action(self, state_index):\n",
    "        \"\"\"\n",
    "        Select an action using epsilon-greedy strategy.\n",
    "        \"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.randint(self.action_size)  # Random action (exploration)\n",
    "        else:\n",
    "            action = np.argmax(self.q_table[state_index])  # Best action (exploitation)\n",
    "        \n",
    "        possible_actions = [\n",
    "            [HGate(), [0]],\n",
    "            [HGate(), [1]],\n",
    "            [CXGate(), [0, 1]],\n",
    "            [CXGate(), [1, 0]],\n",
    "            [TGate(), [0]],\n",
    "            [TGate(), [1]],\n",
    "        ]\n",
    "        \n",
    "        return possible_actions[action], action\n",
    "\n",
    "    def choose_actionNoE(self, state_index):\n",
    "        \"\"\"\n",
    "        Select the best action based on the current Q-table without exploration.\n",
    "        \"\"\"\n",
    "        action = np.argmax(self.q_table[state_index])\n",
    "        \n",
    "        possible_actions = [\n",
    "            [HGate(), [0]],\n",
    "            [HGate(), [1]],\n",
    "            [CXGate(), [0, 1]],\n",
    "            [CXGate(), [1, 0]],\n",
    "            [TGate(), [0]],\n",
    "            [TGate(), [1]],\n",
    "        ]\n",
    "        \n",
    "        return possible_actions[action], action\n",
    "    \n",
    "    def update_q_table(self, state_index, action, reward, next_state_index):\n",
    "        \"\"\"\n",
    "        Update the Q-table using the Q-learning formula.\n",
    "        \"\"\"\n",
    "        self.q_table[state_index, action] += self.alpha * (\n",
    "            reward + self.gamma * np.max(self.q_table[next_state_index]) - self.q_table[state_index, action]\n",
    "        )\n",
    "    \n",
    "    def decay_exploration(self):\n",
    "        \"\"\"\n",
    "        Reduce epsilon value over time to shift from exploration to exploitation.\n",
    "        \"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.decay_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a7ad1c-0325-4fad-ad96-0b35c3e9cb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the Q-learning agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, state_size, action_size, alpha, gamma, epsilon, decay_rate, epsilon_min):\n",
    "        # Initialize the agent's parameters\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.decay_rate = decay_rate\n",
    "        self.epsilon_min = epsilon_min\n",
    "        # Initialize the Q-table with zeros\n",
    "        self.q_table = np.zeros((state_size, action_size))\n",
    "    \n",
    "    def choose_action(self, state_index):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Exploration: random action\n",
    "            action = np.random.randint(self.action_size)\n",
    "        else:\n",
    "            # Exploitation: choose the best action\n",
    "            action = np.argmax(self.q_table[state_index])\n",
    "        \n",
    "        possible_actions = [\n",
    "            [HGate(), [0]],\n",
    "            [HGate(), [1]],\n",
    "            [CXGate(), [0, 1]],\n",
    "            [CXGate(), [1, 0]],\n",
    "            [TdgGate(), [0]],\n",
    "            [TdgGate(), [1]],\n",
    "        ]\n",
    "        \n",
    "        return possible_actions[action],action\n",
    "    \n",
    "    def choose_actionNoE(self, state_index):\n",
    "       \n",
    "        action = np.argmax(self.q_table[state_index])\n",
    "        \n",
    "        possible_actions = [\n",
    "            [HGate(), [0]],\n",
    "            [HGate(), [1]],\n",
    "            [CXGate(), [0, 1]],\n",
    "            [CXGate(), [1, 0]],\n",
    "            [TdgGate(), [0]],\n",
    "            [TdgGate(), [1]],\n",
    "        ]\n",
    "        \n",
    "        return possible_actions[action],action\n",
    "    \n",
    "    def update_q_table(self, state_index, action, reward, next_state_index):\n",
    "        # Update the Q-table based on the agent's experience\n",
    "        self.q_table[state_index, action] += self.alpha * (\n",
    "            reward + self.gamma * np.max(self.q_table[next_state_index]) - self.q_table[state_index, action]\n",
    "        )\n",
    "    \n",
    "    def decay_exploration(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.decay_rate)\n",
    "\n",
    "# Train the agent\n",
    "def train_agent(agent, environment, episodes, max_steps_per_episode):\n",
    "    for episode in range(episodes):\n",
    "        # Reset the environment at the beginning of each episode\n",
    "        state_index = environment.reset()\n",
    "        episode_reward = 0\n",
    "        for step in range(max_steps_per_episode):\n",
    "            # Choose an action\n",
    "            action,action_index = agent.choose_action(state_index)\n",
    "            \n",
    "            # Take the action and observe the outcome\n",
    "            next_state_index, reward, done = environment.step(action[0],action[1])\n",
    "            episode_reward += reward \n",
    "            # Update the Q-table\n",
    "            agent.update_q_table(state_index, action_index, reward, next_state_index)\n",
    "            \n",
    "            # Update the state\n",
    "            state_index = next_state_index\n",
    "            \n",
    "            # Check if the episode is done\n",
    "            if done:\n",
    "                print(\"Generated circuit:\")\n",
    "                environment.render()\n",
    "                print(f\"Episode {episode + 1}: Total Reward = {episode_reward}\")\n",
    "                break\n",
    "            if environment.circuit.size() > 4:\n",
    "                episode_reward -= 100  # Negative reward for exceeding maximum gates\n",
    "                break\n",
    "        \n",
    "        # Decay the exploration rate\n",
    "         # Save results every 100 attempts\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episode {episode + 1}: Total Reward = {episode_reward}\")\n",
    "        agent.decay_exploration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae60713",
   "metadata": {},
   "source": [
    "# **3:Train the Agent**\n",
    "\n",
    "The `train_agent` function is responsible for training a reinforcement learning (RL) agent to optimize decision-making in a given environment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2222e852-009a-43f8-b3c4-6750937316cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "def train_agent(agent, environment, episodes, max_steps_per_episode):\n",
    "    for episode in range(episodes):\n",
    "        # Reset the environment at the beginning of each episode\n",
    "        state_index = environment.reset()\n",
    "        episode_reward = 0  # Initialize the total reward for this episode\n",
    "        \n",
    "        for step in range(max_steps_per_episode):\n",
    "            # Choose an action using the agent's policy\n",
    "            action, action_index = agent.choose_action(state_index)\n",
    "            \n",
    "            # Execute the chosen action and observe the outcome\n",
    "            next_state_index, reward, done = environment.step(action[0], action[1])\n",
    "            episode_reward += reward  # Accumulate the reward\n",
    "            \n",
    "            # Update the Q-table based on the agent's learning algorithm\n",
    "            agent.update_q_table(state_index, action_index, reward, next_state_index)\n",
    "            \n",
    "            # Update the current state\n",
    "            state_index = next_state_index\n",
    "            \n",
    "            # Check if the episode has reached a terminal state\n",
    "            if done:\n",
    "                print(\"Generated circuit:\")\n",
    "                environment.render()  # Render the environment to visualize the result\n",
    "                print(f\"Episode {episode + 1}: Total Reward = {episode_reward}\")\n",
    "                break\n",
    "            \n",
    "            # Apply a penalty if the circuit exceeds the maximum number of allowed gates\n",
    "            if environment.circuit.size() > 4:\n",
    "                episode_reward -= 100  # Negative reward for exceeding the maximum gate limit\n",
    "                break\n",
    "        \n",
    "        # Print results every 100 episodes\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episode {episode + 1}: Total Reward = {episode_reward}\")\n",
    "        \n",
    "        # Decay the exploration rate to encourage exploitation over time\n",
    "        agent.decay_exploration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c5085a",
   "metadata": {},
   "source": [
    "# **4:Test the Trained Agent Without Exploration**\n",
    "\n",
    "The `test_agent` function evaluates a trained reinforcement learning (RL) agent by running it in the environment **without exploration** (i.e., the agent strictly follows the learned policy).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acf932ca-d86a-4ac8-97af-b69b3204b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained agent without exploration\n",
    "def test_agent(agent, environment, episodes, max_steps_per_episode):\n",
    "    for episode in range(episodes):\n",
    "        # Reset the environment\n",
    "        environment.reset()\n",
    "        state_index = environment.reset()\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "            # Choose an action based purely on learned policy (no exploration)\n",
    "            action, action_index = agent.choose_actionNoE(state_index)\n",
    "            \n",
    "            # Execute the chosen action and observe the outcome\n",
    "            next_state_index, reward, done = environment.step(action[0], action[1])\n",
    "            \n",
    "            # Update the current state\n",
    "            state_index = next_state_index\n",
    "            \n",
    "            # Check if the episode has reached a terminal state\n",
    "            if done:\n",
    "                global holder\n",
    "                holder += 1  # Increment success counter\n",
    "                break\n",
    "        \n",
    "        # Render the environment to visualize the test result\n",
    "        environment.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a324dca3",
   "metadata": {},
   "source": [
    "\n",
    "# **5:Main Function**\n",
    "\n",
    "The `__main__` block initializes and trains a reinforcement learning agent multiple times, followed by testing its performance. The learning effect of the Q-learning agent in the custom quantum environment is evaluated by repeating the training and testing process 20 times. In each iteration, the agent learns the strategy within 100 episodes, and then verifies its performance in 1 test. If successful, it is counted into the global counter holder, and finally the average success rate is output to measure the stability and effectiveness of the strategy.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42fda2df-a893-43cf-8b02-9997d3548e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 5: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 13: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 45: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 47: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 48: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 53: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 54: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 55: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 58: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 59: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 60: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 67: Total Reward = 100\n",
      "                    ┌───┐\n",
      "q_0: ───────■────■──┤ X ├\n",
      "     ┌───┐┌─┴─┐┌─┴─┐└─┬─┘\n",
      "q_1: ┤ H ├┤ X ├┤ X ├──■──\n",
      "     └───┘└───┘└───┘     \n",
      "Generated circuit:\n",
      "                    ┌───┐\n",
      "q_0: ───────■────■──┤ X ├\n",
      "     ┌───┐┌─┴─┐┌─┴─┐└─┬─┘\n",
      "q_1: ┤ H ├┤ X ├┤ X ├──■──\n",
      "     └───┘└───┘└───┘     \n",
      "Episode 73: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 78: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 80: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 88: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 91: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 92: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 96: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 97: Total Reward = 100\n",
      "                    ┌───┐\n",
      "q_0: ───────■────■──┤ X ├\n",
      "     ┌───┐┌─┴─┐┌─┴─┐└─┬─┘\n",
      "q_1: ┤ H ├┤ X ├┤ X ├──■──\n",
      "     └───┘└───┘└───┘     \n",
      "Generated circuit:\n",
      "                    ┌───┐\n",
      "q_0: ───────■────■──┤ X ├\n",
      "     ┌───┐┌─┴─┐┌─┴─┐└─┬─┘\n",
      "q_1: ┤ H ├┤ X ├┤ X ├──■──\n",
      "     └───┘└───┘└───┘     \n",
      "Episode 100: Total Reward = 100\n",
      "Episode 100: Total Reward = 100\n",
      "Test Result\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 17: Total Reward = 100\n",
      "     ┌───┐┌───┐┌───┐\n",
      "q_0: ┤ H ├┤ H ├┤ X ├\n",
      "     ├───┤└───┘└─┬─┘\n",
      "q_1: ┤ H ├───────■──\n",
      "     └───┘          \n",
      "Generated circuit:\n",
      "     ┌───┐┌───┐┌───┐\n",
      "q_0: ┤ H ├┤ H ├┤ X ├\n",
      "     ├───┤└───┘└─┬─┘\n",
      "q_1: ┤ H ├───────■──\n",
      "     └───┘          \n",
      "Episode 24: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 30: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 32: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 35: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 42: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 58: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 62: Total Reward = 100\n",
      "                    ┌───┐\n",
      "q_0: ──■────■───────┤ X ├\n",
      "     ┌─┴─┐┌─┴─┐┌───┐└─┬─┘\n",
      "q_1: ┤ X ├┤ X ├┤ H ├──■──\n",
      "     └───┘└───┘└───┘     \n",
      "Generated circuit:\n",
      "                    ┌───┐\n",
      "q_0: ──■────■───────┤ X ├\n",
      "     ┌─┴─┐┌─┴─┐┌───┐└─┬─┘\n",
      "q_1: ┤ X ├┤ X ├┤ H ├──■──\n",
      "     └───┘└───┘└───┘     \n",
      "Episode 64: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 68: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 70: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 71: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 73: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 78: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 81: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 83: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 84: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 85: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 86: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 89: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 98: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 99: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 100: Total Reward = 100\n",
      "Episode 100: Total Reward = 100\n",
      "Test Result\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 6: Total Reward = 100\n",
      "     ┌───┐┌───┐┌───┐\n",
      "q_0: ┤ H ├┤ H ├┤ X ├\n",
      "     ├───┤└───┘└─┬─┘\n",
      "q_1: ┤ H ├───────■──\n",
      "     └───┘          \n",
      "Generated circuit:\n",
      "     ┌───┐┌───┐┌───┐\n",
      "q_0: ┤ H ├┤ H ├┤ X ├\n",
      "     ├───┤└───┘└─┬─┘\n",
      "q_1: ┤ H ├───────■──\n",
      "     └───┘          \n",
      "Episode 8: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 32: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 36: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 37: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 48: Total Reward = 100\n",
      "     ┌───┐┌───┐┌───┐\n",
      "q_0: ┤ H ├┤ H ├┤ X ├\n",
      "     ├───┤└───┘└─┬─┘\n",
      "q_1: ┤ H ├───────■──\n",
      "     └───┘          \n",
      "Generated circuit:\n",
      "     ┌───┐┌───┐┌───┐\n",
      "q_0: ┤ H ├┤ H ├┤ X ├\n",
      "     ├───┤└───┘└─┬─┘\n",
      "q_1: ┤ H ├───────■──\n",
      "     └───┘          \n",
      "Episode 50: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 53: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 56: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 60: Total Reward = 100\n",
      "     ┌───┐┌───┐┌───┐\n",
      "q_0: ┤ H ├┤ H ├┤ X ├\n",
      "     ├───┤└───┘└─┬─┘\n",
      "q_1: ┤ H ├───────■──\n",
      "     └───┘          \n",
      "Generated circuit:\n",
      "     ┌───┐┌───┐┌───┐\n",
      "q_0: ┤ H ├┤ H ├┤ X ├\n",
      "     ├───┤└───┘└─┬─┘\n",
      "q_1: ┤ H ├───────■──\n",
      "     └───┘          \n",
      "Episode 62: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 64: Total Reward = 100\n",
      "     ┌───┐┌───┐┌───┐\n",
      "q_0: ┤ H ├┤ H ├┤ X ├\n",
      "     ├───┤└───┘└─┬─┘\n",
      "q_1: ┤ H ├───────■──\n",
      "     └───┘          \n",
      "Generated circuit:\n",
      "     ┌───┐┌───┐┌───┐\n",
      "q_0: ┤ H ├┤ H ├┤ X ├\n",
      "     ├───┤└───┘└─┬─┘\n",
      "q_1: ┤ H ├───────■──\n",
      "     └───┘          \n",
      "Episode 65: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 72: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 75: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 77: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 79: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 81: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 85: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 87: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 89: Total Reward = 100\n",
      "                    ┌───┐\n",
      "q_0: ───────■────■──┤ X ├\n",
      "     ┌───┐┌─┴─┐┌─┴─┐└─┬─┘\n",
      "q_1: ┤ H ├┤ X ├┤ X ├──■──\n",
      "     └───┘└───┘└───┘     \n",
      "Generated circuit:\n",
      "                    ┌───┐\n",
      "q_0: ───────■────■──┤ X ├\n",
      "     ┌───┐┌─┴─┐┌─┴─┐└─┬─┘\n",
      "q_1: ┤ H ├┤ X ├┤ X ├──■──\n",
      "     └───┘└───┘└───┘     \n",
      "Episode 90: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 95: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 98: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 99: Total Reward = 100\n",
      "Episode 100: Total Reward = -100\n",
      "Test Result\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 25: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 27: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 28: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 29: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 48: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 49: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 50: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 59: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 62: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 63: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 71: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 76: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 79: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 86: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 90: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 91: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 92: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 93: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 94: Total Reward = 100\n",
      "     ┌───┐┌───┐┌───┐\n",
      "q_0: ┤ H ├┤ H ├┤ X ├\n",
      "     ├───┤└───┘└─┬─┘\n",
      "q_1: ┤ H ├───────■──\n",
      "     └───┘          \n",
      "Generated circuit:\n",
      "     ┌───┐┌───┐┌───┐\n",
      "q_0: ┤ H ├┤ H ├┤ X ├\n",
      "     ├───┤└───┘└─┬─┘\n",
      "q_1: ┤ H ├───────■──\n",
      "     └───┘          \n",
      "Episode 96: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 98: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 99: Total Reward = 100\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Generated circuit:\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 100: Total Reward = 100\n",
      "Episode 100: Total Reward = 100\n",
      "Test Result\n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "          ┌───┐\n",
      "q_0: ─────┤ X ├\n",
      "     ┌───┐└─┬─┘\n",
      "q_1: ┤ H ├──■──\n",
      "     └───┘     \n",
      "Episode 100: Total Reward = -100\n",
      "Test Result\n",
      "     ┌───┐┌───┐┌───┐┌───┐┌───┐\n",
      "q_0: ┤ H ├┤ H ├┤ H ├┤ H ├┤ H ├\n",
      "     └───┘└───┘└───┘└───┘└───┘\n",
      "q_1: ─────────────────────────\n",
      "                              \n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "global holder\n",
    "holder = 0  # Initialize success counter\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    # Run multiple training and testing iterations\n",
    "    for i in range(5):\n",
    "        # Initialize the environment and agent for each iteration\n",
    "        environment = QuantumEnv()\n",
    "        agent = QLearningAgent(state_size=100, action_size=6, alpha=0.1, gamma=0.95, epsilon=1, decay_rate=0.99, epsilon_min=0.05)\n",
    "        \n",
    "        # Train the agent\n",
    "        train_agent(agent, environment, episodes=100, max_steps_per_episode=5)\n",
    "        \n",
    "        # Test the trained agent\n",
    "        print(\"Test Result\")\n",
    "        test_agent(agent, environment, episodes=1, max_steps_per_episode=5)\n",
    "    \n",
    "    # Print the average success rate over 20 iterations\n",
    "    print(holder / 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
