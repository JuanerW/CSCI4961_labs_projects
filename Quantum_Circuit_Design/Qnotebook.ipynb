{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b554e55-6654-4815-8a68-66511847deaf",
   "metadata": {},
   "source": [
    "Using reinforcement learning to optimize decision-making strategies for quantum circuit design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d9459a-5f98-42a6-b497-a81a38f07896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from qiskit import QuantumCircuit, transpile\n",
    "from qiskit_aer import Aer\n",
    "from qiskit.circuit.library import HGate, CXGate, SGate, TGate, XGate, YGate, ZGate, CRZGate, TdgGate, UnitaryGate\n",
    "from qiskit.quantum_info import Operator\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df169982-e1ae-48ab-841a-d14b1b4f17e6",
   "metadata": {},
   "source": [
    "Basic Quantum Gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d635c3d6-2764-4851-afbe-1eb0af7f21d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = np.array([[1, 1], [1, -1]]) / np.sqrt(2)\n",
    "X = np.array([[0, 1], [1, 0]])\n",
    "Z = np.array([[1, 0], [0, -1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a4783e-3fb4-48b8-814c-2cbb1a8824e3",
   "metadata": {},
   "source": [
    "Some object circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51c38e94-fa02-4dc2-95f7-d70df2775aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define matrices and operators\n",
    "swap_matrix = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 0, 1]\n",
    "])\n",
    "\n",
    "CNOT = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 0, 1],\n",
    "    [0, 0, 1, 0]\n",
    "])\n",
    "\n",
    "# Bell state unitary\n",
    "bell_state_unitary = Operator(CNOT) @ Operator(np.kron(H, np.eye(2)))\n",
    "phi_minus = Operator(np.kron(np.eye(2), Z)) @ Operator(CNOT) @ Operator(np.kron(H, np.eye(2)))\n",
    "psi_plus = Operator(CNOT) @ Operator(np.kron(X, np.eye(2))) @ Operator(np.kron(H, np.eye(2)))\n",
    "psi_minus = Operator(np.kron(np.eye(2), Z)) @ Operator(CNOT) @ Operator(np.kron(X, np.eye(2))) @ Operator(np.kron(H, np.eye(2)))\n",
    "\n",
    "# CZ matrix\n",
    "cz_matrix = np.array([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 0, 0, -1]\n",
    "])\n",
    "\n",
    "# GHZ Circuit (3 qubits)\n",
    "ghz_circuit = QuantumCircuit(3)\n",
    "ghz_circuit.h(0)\n",
    "ghz_circuit.cx(0, 1)\n",
    "ghz_circuit.cx(1, 2)\n",
    "ghz_circuit = Operator(ghz_circuit)\n",
    "\n",
    "# Textbook circuits\n",
    "# page 200\n",
    "text_circuit1 = QuantumCircuit(3)\n",
    "text_circuit1.cx(0,1)\n",
    "text_circuit1.cx(1,2)\n",
    "text_circuit1.h(0)\n",
    "text_circuit1.h(1)\n",
    "text_circuit1.h(2)\n",
    "text_circuit1 = Operator(text_circuit1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82345c3-9bea-4c05-a858-e139d1584dc9",
   "metadata": {},
   "source": [
    "You can make your own circuit here and modify and test the effect of Q learning reinforcement learning algorithm in designing the circuit in the subsequent code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec0e69f-a1f3-4dd4-8a51-9dcfc3692eb0",
   "metadata": {},
   "source": [
    "Hash function for Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebaf3c1-400f-4000-b6b3-3c1560588c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store unique matrix hashes and their corresponding IDs\n",
    "matrix_dict = {}\n",
    "counter = 0 \n",
    "\n",
    "def matrix_to_hash(matrix):\n",
    "    \"\"\"\n",
    "    Convert a matrix to a hashable tuple format.\n",
    "    \"\"\"\n",
    "    matrix_array = np.asarray(matrix) \n",
    "    return tuple(tuple(row) for row in matrix_array)\n",
    "\n",
    "def get_matrix_id(matrix):\n",
    "    \"\"\"\n",
    "    Assign a unique ID to a matrix if it has not been encountered before.\n",
    "    \"\"\"\n",
    "    global counter\n",
    "    matrix_hash = matrix_to_hash(matrix)\n",
    "    \n",
    "    if matrix_hash not in matrix_dict:\n",
    "        matrix_dict[matrix_hash] = counter\n",
    "        counter += 1  \n",
    "    \n",
    "    return matrix_dict[matrix_hash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4012e89e-ce2c-4990-b0dd-3e6c293573cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from qiskit import QuantumCircuit, Aer, transpile\n",
    "from qiskit.quantum_info import Operator\n",
    "import numpy as np\n",
    "\n",
    "class QuantumEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(QuantumEnv, self).__init__()\n",
    "        \n",
    "        # Set the number of qubits\n",
    "        self.num_qubits = 2\n",
    "        # Initialize the quantum circuit\n",
    "        self.circuit = QuantumCircuit(self.num_qubits)\n",
    "        # Set the target unitary matrix (can be changed to bell_state, cz, swap, iswap)\n",
    "        self.target_unitary = iswap_matrix  \n",
    "        \n",
    "        # Define the action space (6 possible actions)\n",
    "        self.action_space = spaces.Discrete(6)\n",
    "        # Define the observation space (100 possible state hashes)\n",
    "        self.observation_space = spaces.Discrete(100)\n",
    "        \n",
    "        # Mapping of state indices\n",
    "        self.state_to_index = {}\n",
    "        self.index_to_state = []\n",
    "\n",
    "    def _hash_circuit(self, circuit: QuantumCircuit) -> int:\n",
    "        \"\"\"\n",
    "        Compute a hash value for the given quantum circuit.\n",
    "        \"\"\"\n",
    "        matrix = Operator(circuit)  # Get the unitary matrix of the circuit\n",
    "        return get_matrix_id(matrix) % 100  # Compute hash value within 100\n",
    "\n",
    "    def get_state_index(self, state: QuantumCircuit) -> int:\n",
    "        \"\"\"\n",
    "        Get the index of a state; if it is a new state, add it to the index mapping.\n",
    "        \"\"\"\n",
    "        state_hash = self._hash_circuit(state)\n",
    "        if state_hash not in self.state_to_index:\n",
    "            index = len(self.state_to_index)\n",
    "            self.state_to_index[state_hash] = index\n",
    "            self.index_to_state.append(state)\n",
    "        return self.state_to_index[state_hash]\n",
    "\n",
    "    def get_state_from_index(self, index: int) -> QuantumCircuit:\n",
    "        \"\"\"\n",
    "        Retrieve the quantum circuit state based on the index.\n",
    "        \"\"\"\n",
    "        if 0 <= index < len(self.index_to_state):\n",
    "            return self.index_to_state[index]\n",
    "        return None\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment and return the initial state index.\n",
    "        \"\"\"\n",
    "        self.circuit = QuantumCircuit(self.num_qubits)  # Reinitialize the circuit\n",
    "        return self.get_state_index(self.circuit)\n",
    "\n",
    "    def step(self, action, qubits):\n",
    "        \"\"\"\n",
    "        Execute an action, update the environment state, and compute the reward.\n",
    "        \"\"\"\n",
    "        self.circuit.append(action, qubits)  # Append the action to the circuit\n",
    "        state_index = self.get_state_index(self.circuit)  # Get the new state index\n",
    "        reward, done = self._reward(self.target_unitary)  # Compute the reward\n",
    "        return state_index, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Render the quantum circuit.\n",
    "        \"\"\"\n",
    "        print(self.circuit.draw())\n",
    "\n",
    "    def _reward(self, target_unitary):\n",
    "        \"\"\"\n",
    "        Compute the fidelity between the circuit and the target unitary matrix and return the reward.\n",
    "        \"\"\"\n",
    "        simulator = Aer.get_backend('unitary_simulator')  # Get the unitary simulator\n",
    "        result = simulator.run(transpile(self.circuit, simulator)).result()\n",
    "        unitary = result.get_unitary(self.circuit)  # Get the unitary matrix of the current circuit\n",
    "        \n",
    "        # Compute the fidelity of the quantum state\n",
    "        unitary_array = np.asarray(unitary)\n",
    "        target_unitary_array = np.asarray(target_unitary)\n",
    "        fidelity = np.abs(np.trace(unitary_array.conj().T @ target_unitary_array)) / (2 ** self.num_qubits)\n",
    "        \n",
    "        reward = 0\n",
    "        done = False\n",
    "        if fidelity > 0.99:\n",
    "            done = True  # Task completed\n",
    "            reward += 100  # Assign high reward\n",
    "            self.render()  # Display the final circuit\n",
    "        return reward, done\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Close the environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Display the quantum circuit.\n",
    "        \"\"\"\n",
    "        print(self.circuit.draw())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c993060c-f3a6-4fcf-a3f1-f4ef1f0207ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-learning agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, state_size, action_size, alpha, gamma, epsilon, decay_rate, epsilon_min):\n",
    "        \"\"\"\n",
    "        Initialize the Q-learning agent with given parameters.\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.decay_rate = decay_rate  # Decay rate for epsilon\n",
    "        self.epsilon_min = epsilon_min  # Minimum value of epsilon\n",
    "        self.q_table = np.zeros((state_size, action_size))  # Initialize Q-table with zeros\n",
    "    \n",
    "    def choose_action(self, state_index):\n",
    "        \"\"\"\n",
    "        Select an action using epsilon-greedy strategy.\n",
    "        \"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.randint(self.action_size)  # Random action (exploration)\n",
    "        else:\n",
    "            action = np.argmax(self.q_table[state_index])  # Best action (exploitation)\n",
    "        \n",
    "        possible_actions = [\n",
    "            [HGate(), [0]],\n",
    "            [HGate(), [1]],\n",
    "            [CXGate(), [0, 1]],\n",
    "            [CXGate(), [1, 0]],\n",
    "            [TGate(), [0]],\n",
    "            [TGate(), [1]],\n",
    "        ]\n",
    "        \n",
    "        return possible_actions[action], action\n",
    "\n",
    "    def choose_actionNoE(self, state_index):\n",
    "        \"\"\"\n",
    "        Select the best action based on the current Q-table without exploration.\n",
    "        \"\"\"\n",
    "        action = np.argmax(self.q_table[state_index])\n",
    "        \n",
    "        possible_actions = [\n",
    "            [HGate(), [0]],\n",
    "            [HGate(), [1]],\n",
    "            [CXGate(), [0, 1]],\n",
    "            [CXGate(), [1, 0]],\n",
    "            [TGate(), [0]],\n",
    "            [TGate(), [1]],\n",
    "        ]\n",
    "        \n",
    "        return possible_actions[action], action\n",
    "    \n",
    "    def update_q_table(self, state_index, action, reward, next_state_index):\n",
    "        \"\"\"\n",
    "        Update the Q-table using the Q-learning formula.\n",
    "        \"\"\"\n",
    "        self.q_table[state_index, action] += self.alpha * (\n",
    "            reward + self.gamma * np.max(self.q_table[next_state_index]) - self.q_table[state_index, action]\n",
    "        )\n",
    "    \n",
    "    def decay_exploration(self):\n",
    "        \"\"\"\n",
    "        Reduce epsilon value over time to shift from exploration to exploitation.\n",
    "        \"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.decay_rate)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
